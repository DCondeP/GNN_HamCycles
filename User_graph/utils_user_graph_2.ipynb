{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from itertools import chain, islice\n",
    "from time import time\n",
    "\n",
    "\n",
    "# GNN class to be instantiated with specified param values\n",
    "class GCN_dev(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, number_classes, dropout, device):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of the core GCN model of provided size.\n",
    "        Dropout is added in forward step.\n",
    "\n",
    "        Inputs:\n",
    "            in_feats: Dimension of the input (embedding) layer\n",
    "            hidden_size: Hidden layer size\n",
    "            dropout: Fraction of dropout to add between intermediate layer. Value is cached for later use.\n",
    "            device: Specifies device (CPU vs GPU) to load variables onto\n",
    "        \"\"\"\n",
    "        super(GCN_dev, self).__init__()\n",
    "\n",
    "        self.dropout_frac = dropout\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size).to(device)\n",
    "        self.conv2 = GraphConv(hidden_size, number_classes).to(device)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        \"\"\"\n",
    "        Run forward propagation step of instantiated model.\n",
    "\n",
    "        Input:\n",
    "            self: GCN_dev instance\n",
    "            g: DGL graph object, i.e. problem definition\n",
    "            inputs: Input (embedding) layer weights, to be propagated through network\n",
    "        Output:\n",
    "            h: Output layer weights\n",
    "        \"\"\"\n",
    "\n",
    "        # input step\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout_frac)\n",
    "\n",
    "        # output step\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.sigmoid(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5452e23",
   "metadata": {},
   "source": [
    "Generate specific graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c49471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(nodes, edges):\n",
    "    nx_graph = nx.OrderedGraph()\n",
    "    \n",
    "    \n",
    "    for nodo in nodes:\n",
    "        nx_graph.add_node(nodo)\n",
    "    \n",
    "    nx_graph.add_edges_from(edges)\n",
    "    \n",
    "    return nx_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37610b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert Q dictionary to torch tensor\n",
    "def qubo_dict_to_torch(nx_G, Q, torch_dtype=None, torch_device=None):\n",
    "    \"\"\"\n",
    "    Output Q matrix as torch tensor for given Q in dictionary format.\n",
    "\n",
    "    Input:\n",
    "        Q: QUBO matrix as defaultdict\n",
    "        nx_G: graph as networkx object (needed for node lables can vary 0,1,... vs 1,2,... vs a,b,...)\n",
    "    Output:\n",
    "        Q: QUBO as torch tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # get number of nodes\n",
    "    n_nodes = len(nx_G.nodes)\n",
    "\n",
    "    # get QUBO Q as torch tensor\n",
    "    Q_mat = torch.zeros(n_nodes, n_nodes)\n",
    "    for (x_coord, y_coord), val in Q.items():\n",
    "        Q_mat[x_coord][y_coord] = val\n",
    "\n",
    "    if torch_dtype is not None:\n",
    "        Q_mat = Q_mat.type(torch_dtype)\n",
    "\n",
    "    if torch_device is not None:\n",
    "        Q_mat = Q_mat.to(torch_device)\n",
    "\n",
    "    return Q_mat\n",
    "\n",
    "\n",
    "\n",
    "# Chunk long list\n",
    "def gen_combinations(combs, chunk_size):\n",
    "    yield from iter(lambda: list(islice(combs, chunk_size)), [])\n",
    "\n",
    "\n",
    "# helper function for custom loss according to Q matrix\n",
    "def loss_func(probs, Q_mat):\n",
    "    \"\"\"\n",
    "    Function to compute cost value for given probability of spin [prob(+1)] and predefined Q matrix.\n",
    "\n",
    "    Input:\n",
    "        probs: Probability of each node belonging to each class, as a vector\n",
    "        Q_mat: QUBO as torch tensor\n",
    "    \"\"\"\n",
    "\n",
    "    probs_ = torch.unsqueeze(probs, 1)\n",
    "\n",
    "    # minimize cost = x.T * Q * x\n",
    "    cost = (probs_.T @ Q_mat @ probs_).squeeze()\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Construct graph to learn on\n",
    "def get_gnn(n_nodes, gnn_hypers, opt_params, torch_device, torch_dtype):\n",
    "    \"\"\"\n",
    "    Generate GNN instance with specified structure. Creates GNN, retrieves embedding layer,\n",
    "    and instantiates ADAM optimizer given those.\n",
    "\n",
    "    Input:\n",
    "        n_nodes: Problem size (number of nodes in graph)\n",
    "        gnn_hypers: Hyperparameters relevant to GNN structure\n",
    "        opt_params: Hyperparameters relevant to ADAM optimizer\n",
    "        torch_device: Whether to load pytorch variables onto CPU or GPU\n",
    "        torch_dtype: Datatype to use for pytorch variables\n",
    "    Output:\n",
    "        net: GNN instance\n",
    "        embed: Embedding layer to use as input to GNN\n",
    "        optimizer: ADAM optimizer instance\n",
    "    \"\"\"\n",
    "    dim_embedding = gnn_hypers['dim_embedding']\n",
    "    hidden_dim = gnn_hypers['hidden_dim']\n",
    "    dropout = gnn_hypers['dropout']\n",
    "    number_classes = gnn_hypers['number_classes']\n",
    "\n",
    "    # instantiate the GNN\n",
    "    net = GCN_dev(dim_embedding, hidden_dim, number_classes, dropout, torch_device)\n",
    "    net = net.type(torch_dtype).to(torch_device)\n",
    "    embed = nn.Embedding(n_nodes, dim_embedding)\n",
    "    embed = embed.type(torch_dtype).to(torch_device)\n",
    "\n",
    "    # set up Adam optimizer\n",
    "    params = chain(net.parameters(), embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, **opt_params)\n",
    "    return net, embed, optimizer\n",
    "\n",
    "\n",
    "# Parent function to run GNN training given input config\n",
    "def run_gnn_training(q_torch, dgl_graph, net, embed, optimizer, number_epochs, tol, patience, prob_threshold):\n",
    "    \"\"\"\n",
    "    Wrapper function to run and monitor GNN training. Includes early stopping.\n",
    "    \"\"\"\n",
    "    # Assign variable for user reference\n",
    "    inputs = embed.weight\n",
    "\n",
    "    prev_loss = 1.  # initial loss value (arbitrary)\n",
    "    count = 0       # track number times early stopping is triggered\n",
    "\n",
    "    # initialize optimal solution\n",
    "    best_bitstring = torch.zeros((dgl_graph.number_of_nodes(),)).type(q_torch.dtype).to(q_torch.device)\n",
    "    best_loss = loss_func(best_bitstring.float(), q_torch)\n",
    "\n",
    "    t_gnn_start = time()\n",
    "\n",
    "    # Training logic\n",
    "    for epoch in range(number_epochs):\n",
    "\n",
    "        # get logits/activations\n",
    "        probs = net(dgl_graph, inputs)[:, 0]  # collapse extra dimension output from model\n",
    "\n",
    "        # build cost value with QUBO cost function\n",
    "        loss = loss_func(probs, q_torch)\n",
    "        loss_ = loss.detach().item()\n",
    "\n",
    "        # Apply projection\n",
    "        bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_bitstring = bitstring\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss_}')\n",
    "\n",
    "        # early stopping check\n",
    "        # If loss increases or change in loss is too small, trigger\n",
    "        if (abs(loss_ - prev_loss) <= tol) | ((loss_ - prev_loss) > 0):\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "        if count >= patience:\n",
    "            print(f'Stopping early on epoch {epoch} (patience: {patience})')\n",
    "            break\n",
    "\n",
    "        # update loss tracking\n",
    "        prev_loss = loss_\n",
    "\n",
    "        # run optimization with backpropagation\n",
    "        optimizer.zero_grad()  # clear gradient for step\n",
    "        loss.backward()        # calculate gradient through compute graph\n",
    "        optimizer.step()       # take step, update weights\n",
    "\n",
    "    t_gnn = time() - t_gnn_start\n",
    "    print(f'GNN training (n={dgl_graph.number_of_nodes()}) took {round(t_gnn, 3)}')\n",
    "    print(f'GNN final continuous loss: {loss_}')\n",
    "    print(f'GNN best continuous loss: {best_loss}')\n",
    "\n",
    "    final_bitstring = (probs.detach() >= prob_threshold) * 1\n",
    "\n",
    "    return net, epoch, final_bitstring, best_bitstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60efba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be606c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
